{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3a75df-6d17-4e2b-afc5-5f243649b30e",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cbc427-cc96-4c21-9940-58110db3904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0340-d6cf-4970-ab49-81b864a89a9c",
   "metadata": {},
   "source": [
    "### Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2703209e-5db5-41f4-a2a7-4f753b0f50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0956b4b1-e7c4-4d0f-bb7b-394974531ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a08528c-c865-495a-8724-cc45139c70e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12764, 13, 849, 403, 368, 32], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998df360-df1d-4591-9640-ce86fdbdf6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d67eca-01dd-4b13-abcd-5b6d482f6df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12764, 13, 849, 403, 368, 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d0e277-45ff-4213-a3e1-d0fd28f946f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens back into text:  Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab9cea-e0b6-416b-94c7-9a4b0379bfaa",
   "metadata": {},
   "source": [
    "### Tokenize multiple texts at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed1f669-8203-424e-97b3-e0af29cd1bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded several texts:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, how are you?\", \"I'm good\", \"Yes\"]\n",
    "encoded_texts = tokenizer(list_texts)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934efc4-6d0e-4a82-bcdc-c91cb7b31d15",
   "metadata": {},
   "source": [
    "### Padding and truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da48d2d5-aec1-4a80-8d52-a85918ec6302",
   "metadata": {},
   "source": [
    "For model, everything in a batch is in a same length because you're operating with fixed size tensor. So, we need to use a strategy called 'padding', to handle the variables' length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc2cbb5-12cd-4d88-b243-01dfb7febc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padding:  [[12764, 13, 849, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [4374, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "encoded_texts_longest = tokenizer(list_texts, padding=True)\n",
    "print(\"Using padding: \", encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5db730-658d-4434-9148-735331006d6f",
   "metadata": {},
   "source": [
    "Truncation is a strategy to make the encoded texts shorter to fit into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0a99b6-ff1a-45b0-8c3a-da0ae61cfdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using truncation:  [[12764, 13, 849], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69626d1-c6b7-4a22-aa6a-b02fa9a813f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using left-side truncation:  [[403, 368, 32], [42, 1353, 1175], [4374]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Using left-side truncation: \", encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4547c755-e654-4e37-ba75-fbce5f4240f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [4374, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"Using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6244b5-da01-44f8-a7e6-c0a178ef379b",
   "metadata": {},
   "source": [
    "### Prepare instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b7796f-d1ed-4ef5-97f2-6f2fe76f2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('kotzeje/lamini_docs.jsonl')\n",
    "train_data = dataset['train']\n",
    "\n",
    "instruction_dataset_df = pd.DataFrame(train_data)\n",
    "\n",
    "examples = instruction_dataset_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25840186-5767-4303-837f-a858e526dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset:\n",
      "{'answer': 'There are several metrics that can be used to evaluate the '\n",
      "           'performance and quality of generated text from Lamini models, '\n",
      "           'including perplexity, BLEU score, and human evaluation. Perplexity '\n",
      "           'measures how well the model predicts the next word in a sequence, '\n",
      "           'while BLEU score measures the similarity between the generated '\n",
      "           'text and a reference text. Human evaluation involves having human '\n",
      "           'judges rate the quality of the generated text based on factors '\n",
      "           'such as coherence, fluency, and relevance. It is recommended to '\n",
      "           'use a combination of these metrics for a comprehensive evaluation '\n",
      "           \"of the model's performance.\",\n",
      " 'question': '### Question:\\n'\n",
      "             'How can I evaluate the performance and quality of the generated '\n",
      "             'text from Lamini models?\\n'\n",
      "             '\\n'\n",
      "             '### Answer:'}\n"
     ]
    }
   ],
   "source": [
    "if \"question\" in examples and \"answer\" in examples:\n",
    "  text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "  text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "  text = examples[\"text\"][0]\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"question\"][i]\n",
    "  answer = examples[\"answer\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"One datapoint in the finetuning dataset:\")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da188f-fce7-42d7-944c-c3c920ffbbb0",
   "metadata": {},
   "source": [
    "### Tokenize a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be2ec5b6-9108-4c93-af3f-9899e226d646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   187  2347   476   309  7472   253  3045   285  3290\n",
      "    273   253  4561  2505   432   418  4988    74  3210    32   187   187\n",
      "   4118 37741    27  2512   403  2067 17082   326   476   320   908   281\n",
      "   7472   253  3045   285  3290   273  4561  2505   432   418  4988    74\n",
      "   3210    13  1690 44229   414    13   378  1843    54  4868    13   285\n",
      "   1966  7103    15  3545 12813   414  5593   849   973   253  1566 26295\n",
      "    253  1735  3159   275   247  3425    13  1223   378  1843    54  4868\n",
      "   5593   253 14259   875   253  4561  2505   285   247  3806  2505    15\n",
      "   8801  7103  8687  1907  1966 16006  2281   253  3290   273   253  4561\n",
      "   2505  1754   327  2616   824   347 25253    13  2938  1371    13   285\n",
      "  17200    15   733   310  8521   281   897   247  5019   273   841 17082\n",
      "    323   247 11088  7103   273   253  1566   434  3045    15]]\n"
     ]
    }
   ],
   "source": [
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf39d50b-1b7c-4179-b8fd-c21cbdf817c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865b6a84-3e64-48d5-a330-6b742de0e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5be5c10-f58b-46a0-8532-2b3ef5f764e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   187,  2347,   476,   309,  7472,   253,\n",
       "         3045,   285,  3290,   273,   253,  4561,  2505,   432,   418,\n",
       "         4988,    74,  3210,    32,   187,   187,  4118, 37741,    27,\n",
       "         2512,   403,  2067, 17082,   326,   476,   320,   908,   281,\n",
       "         7472,   253,  3045,   285,  3290,   273,  4561,  2505,   432,\n",
       "          418,  4988,    74,  3210,    13,  1690, 44229,   414,    13,\n",
       "          378,  1843,    54,  4868,    13,   285,  1966,  7103,    15,\n",
       "         3545, 12813,   414,  5593,   849,   973,   253,  1566, 26295,\n",
       "          253,  1735,  3159,   275,   247,  3425,    13,  1223,   378,\n",
       "         1843,    54,  4868,  5593,   253, 14259,   875,   253,  4561,\n",
       "         2505,   285,   247,  3806,  2505,    15,  8801,  7103,  8687,\n",
       "         1907,  1966, 16006,  2281,   253,  3290,   273,   253,  4561,\n",
       "         2505,  1754,   327,  2616,   824,   347, 25253,    13,  2938,\n",
       "         1371,    13,   285, 17200,    15,   733,   310,  8521,   281,\n",
       "          897,   247,  5019,   273,   841, 17082,   323,   247, 11088,\n",
       "         7103,   273,   253,  1566,   434,  3045,    15]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8f28b-4637-4d64-8dbb-a6f5e20b0ee9",
   "metadata": {},
   "source": [
    "### Tokenize the instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334c2703-092f-40df-a361-9328d93f0619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55cd1c58-abb8-4612-ba97-07c5e32fa3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb8af81f74040efb5c5f730955758cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"kotzeje/lamini_docs.jsonl\", split=\"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3127ac43-62c1-4be6-99bf-b6bd1fe29a2c",
   "metadata": {},
   "source": [
    "drop_last_batch = True, so the last batch won't be in different size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50092e0d-e398-42bf-8ec6-a0360319ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395e836-cf2f-4804-8c5c-fef0510ceaef",
   "metadata": {},
   "source": [
    "### Prepare test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "640d1f37-b85a-440e-89b2-0e2501c60945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e739164-7abf-4786-abad-d8a10230f4d8",
   "metadata": {},
   "source": [
    "### Some datasets for you to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271da99-0111-4d27-88f7-d409a893822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
    "finetuning_dataset = datasets.load_dataset(finetuning_dataset_path)\n",
    "print(finetuning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3c7e4-16f6-42cb-b715-d1f807e7bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_swift_dataset = \"lamini/taylor_swift\"\n",
    "bts_dataset = \"lamini/bts\"\n",
    "open_llms = \"lamini/open_llms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522ee7d-f753-475c-8323-fa1948d98818",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_swiftie = datasets.load_dataset(taylor_swift_dataset)\n",
    "print(dataset_swiftie[\"train\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26782b97-ccce-4d99-8085-31e4a9d8ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to push your own dataset to your Huggingface hub\n",
    "# !pip install huggingface_hub\n",
    "# !huggingface-cli login\n",
    "# split_dataset.push_to_hub(dataset_path_hf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
